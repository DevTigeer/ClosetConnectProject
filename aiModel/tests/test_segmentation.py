"""
ê°„ë‹¨í•œ Cloth Segmentation í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
- ì´ë¯¸ì§€ íŒŒì¼ì„ ì…ë ¥ë°›ì•„ ì„¸ê·¸ë©˜í…Œì´ì…˜ ìˆ˜í–‰
- ê°ì§€ëœ ì˜ë¥˜ ì•„ì´í…œì„ í¬ë¡­í•˜ì—¬ ì €ì¥
- ê²°ê³¼ë¥¼ í„°ë¯¸ë„ì— ì¶œë ¥

ì‚¬ìš©ë²•:
    python test_segmentation.py <ì´ë¯¸ì§€_ê²½ë¡œ>

ì˜ˆì‹œ:
    python test_segmentation.py test/sample.jpg
"""

import sys
import torch
import numpy as np
from PIL import Image
from pathlib import Path
from transformers import AutoImageProcessor, AutoModelForSemanticSegmentation

# ì˜ìƒ ì¹´í…Œê³ ë¦¬ ì •ì˜
CLOTH_LABELS = {
    0: "background",
    1: "hat",
    2: "hair",
    3: "sunglasses",
    4: "upper-clothes",
    5: "skirt",
    6: "pants",
    7: "dress",
    8: "belt",
    9: "left-shoe",
    10: "right-shoe",
    11: "face",
    12: "left-leg",
    13: "right-leg",
    14: "left-arm",
    15: "right-arm",
    16: "bag",
    17: "scarf"
}

# ì¹´í…Œê³ ë¦¬ ë§¤í•‘
CATEGORY_MAPPING = {
    "upper-clothes": "TOP",
    "dress": "TOP",
    "pants": "BOTTOM",
    "skirt": "BOTTOM",
    "hat": "ACC",
    "bag": "ACC",
    "scarf": "ACC",
    "left-shoe": "SHOES",
    "right-shoe": "SHOES"
}

# ì œì™¸í•  ë¼ë²¨ (ë¨¸ë¦¬ì¹´ë½, ì–¼êµ´, íŒ”/ë‹¤ë¦¬)
EXCLUDE_LABELS = {2, 11, 12, 13, 14, 15}


class SimpleClothSegmentation:
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸš€ Device: {self.device}")

        # Hugging Face ëª¨ë¸ ë¡œë“œ
        model_name = "mattmdjaga/segformer_b2_clothes"
        print(f"ğŸ“¦ Loading model: {model_name}...")

        self.processor = AutoImageProcessor.from_pretrained(model_name)
        self.model = AutoModelForSemanticSegmentation.from_pretrained(model_name)
        self.model.to(self.device)
        self.model.eval()

        print("âœ… Model loaded successfully!\n")

    def segment_image(self, image_path):
        """ì´ë¯¸ì§€ ì„¸ê·¸ë©˜í…Œì´ì…˜ ìˆ˜í–‰"""
        # ì´ë¯¸ì§€ ë¡œë“œ
        image = Image.open(image_path).convert("RGB")
        print(f"ğŸ“· ì…ë ¥ ì´ë¯¸ì§€: {image.size} (WÃ—H)")

        # ì „ì²˜ë¦¬
        inputs = self.processor(
            images=image,
            return_tensors="pt",
            do_rescale=True,
            do_normalize=True
        )
        inputs = {k: v.to(self.device) for k, v in inputs.items()}

        # ì¶”ë¡ 
        print("ğŸ” ì„¸ê·¸ë©˜í…Œì´ì…˜ ìˆ˜í–‰ ì¤‘...")
        with torch.no_grad():
            outputs = self.model(**inputs)

        # í›„ì²˜ë¦¬
        logits = outputs.logits
        upsampled_logits = torch.nn.functional.interpolate(
            logits,
            size=image.size[::-1],
            mode="bilinear",
            align_corners=False
        )

        pred_seg = upsampled_logits.argmax(dim=1)[0].cpu().numpy()

        # ì˜ë¥˜ ì•„ì´í…œ ì¶”ì¶œ
        detected_items = self._extract_cloth_items(image, pred_seg)

        return detected_items

    def _extract_cloth_items(self, image, segmentation_map):
        """ì„¸ê·¸ë©˜í…Œì´ì…˜ ë§µì—ì„œ ì˜ë¥˜ ì•„ì´í…œ ì¶”ì¶œ"""
        unique_labels = np.unique(segmentation_map)
        results = []
        image_np = np.array(image)
        img_height, img_width = image_np.shape[:2]

        print(f"\nğŸ“Š ê°ì§€ëœ ë¼ë²¨: {unique_labels}")
        print(f"{'='*60}")

        for label_id in unique_labels:
            if label_id == 0:  # background ì œì™¸
                continue

            # ë¨¸ë¦¬ì¹´ë½, ì–¼êµ´, íŒ”/ë‹¤ë¦¬ ì œì™¸
            if label_id in EXCLUDE_LABELS:
                continue

            label_name = CLOTH_LABELS.get(label_id, "unknown")

            # ì˜· ê´€ë ¨ ë¼ë²¨ë§Œ ì²˜ë¦¬
            if label_name not in CATEGORY_MAPPING:
                print(f"â­ï¸  {label_name} (ë¼ë²¨ {label_id}): ì˜ë¥˜ ì•„ë‹˜, ê±´ë„ˆëœ€")
                continue

            # ë§ˆìŠ¤í¬ ìƒì„±
            mask = (segmentation_map == label_id).astype(np.uint8)
            area_pixels = np.sum(mask)

            # ë„ˆë¬´ ì‘ì€ ì˜ì—­ ì œì™¸
            if area_pixels < 1000:
                print(f"âš ï¸  {label_name} (ë¼ë²¨ {label_id}): ì˜ì—­ì´ ë„ˆë¬´ ì‘ìŒ ({area_pixels} pixels)")
                continue

            # Bounding box ê³„ì‚°
            rows = np.any(mask, axis=1)
            cols = np.any(mask, axis=0)

            if not rows.any() or not cols.any():
                print(f"âŒ {label_name}: bounding box ê³„ì‚° ì‹¤íŒ¨")
                continue

            rmin, rmax = np.where(rows)[0][[0, -1]]
            cmin, cmax = np.where(cols)[0][[0, -1]]

            bbox_width = cmax - cmin
            bbox_height = rmax - rmin

            # Adaptive Padding
            adaptive_padding = max(5, min(20, int(max(bbox_width, bbox_height) * 0.02)))

            rmin_padded = max(0, rmin - adaptive_padding)
            rmax_padded = min(img_height - 1, rmax + adaptive_padding)
            cmin_padded = max(0, cmin - adaptive_padding)
            cmax_padded = min(img_width - 1, cmax + adaptive_padding)

            # í¬ë¡­
            mask_cropped = mask[rmin_padded:rmax_padded+1, cmin_padded:cmax_padded+1]
            image_cropped = image_np[rmin_padded:rmax_padded+1, cmin_padded:cmax_padded+1]

            # RGBA ì´ë¯¸ì§€ ìƒì„±
            alpha_channel = (mask_cropped * 255).astype(np.uint8)
            image_rgba = np.dstack([image_cropped, alpha_channel])

            cropped_pil = Image.fromarray(image_rgba, mode='RGBA')

            # í’ˆì§ˆ í™•ì¸
            opaque_ratio = np.sum(alpha_channel > 0) / alpha_channel.size

            print(f"\nâœ… {label_name} (ë¼ë²¨ {label_id}):")
            print(f"   - ì˜ì—­: {area_pixels:,} pixels")
            print(f"   - Bbox: ({cmin}, {rmin}) â†’ ({cmax}, {rmax}) [í¬ê¸°: {bbox_width}Ã—{bbox_height}]")
            print(f"   - Padding: {adaptive_padding}px")
            print(f"   - í¬ë¡­ ê²°ê³¼: {image_rgba.shape[1]}Ã—{image_rgba.shape[0]} (WÃ—H)")
            print(f"   - ë¶ˆíˆ¬ëª… ë¹„ìœ¨: {opaque_ratio*100:.1f}%", end="")

            if opaque_ratio < 0.3:
                print(" âš ï¸ (íˆ¬ëª… ì˜ì—­ ê³¼ë„)")
            elif opaque_ratio > 0.8:
                print(" ğŸ‰ (ì–‘í˜¸)")
            else:
                print(" âœ“")

            results.append({
                "label": label_name,
                "label_id": int(label_id),
                "category": CATEGORY_MAPPING.get(label_name, "ACC"),
                "area_pixels": int(area_pixels),
                "bbox": [int(cmin_padded), int(rmin_padded), int(cmax_padded), int(rmax_padded)],
                "cropped_image": cropped_pil,
                "opaque_ratio": float(opaque_ratio)
            })

        return results

    def save_results(self, items, output_dir="test_output", show_priority=True):
        """ê²°ê³¼ ì €ì¥"""
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)

        print(f"\n{'='*60}")
        print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘: {output_path}/")

        # Primary item ì„ íƒ (ìš°ì„ ìˆœìœ„)
        if show_priority and len(items) > 1:
            PRIORITY_ORDER = ["dress", "upper-clothes", "pants", "skirt", "left-shoe", "right-shoe", "bag", "hat"]

            primary_item = None
            for priority_label in PRIORITY_ORDER:
                for item in items:
                    if item["label"] == priority_label:
                        primary_item = item
                        break
                if primary_item:
                    break

            if not primary_item:
                primary_item = max(items, key=lambda x: x["area_pixels"])

            print(f"\nğŸ¯ Primary Item: {primary_item['label']} (ìš°ì„ ìˆœìœ„ ì„ íƒ)")
            print(f"   - ì˜ì—­: {primary_item['area_pixels']:,} pixels")
            print(f"   - ì¹´í…Œê³ ë¦¬: {primary_item['category']}")

            # Primaryë§Œ ì €ì¥
            filename = f"primary_{primary_item['label']}.png"
            filepath = output_path / filename
            primary_item['cropped_image'].save(filepath)
            print(f"   âœ“ {filename}")

            # Additional items
            print(f"\nğŸ“¦ Additional Items ({len(items)-1}ê°œ):")
            for i, item in enumerate(items):
                if item == primary_item:
                    continue
                filename = f"additional_{i+1}_{item['label']}.png"
                filepath = output_path / filename
                item['cropped_image'].save(filepath)
                print(f"   âœ“ {filename} ({item['category']})")
        else:
            # ì „ì²´ ì €ì¥
            for i, item in enumerate(items):
                filename = f"{i+1}_{item['label']}.png"
                filepath = output_path / filename
                item['cropped_image'].save(filepath)
                print(f"   âœ“ {filename} ({item['category']})")

        print(f"\nâœ… ì´ {len(items)}ê°œ íŒŒì¼ ì €ì¥ ì™„ë£Œ!")


def main():

    image_path = "/Users/grail/Documents/ClosetConnectProject/aiModel/test/ìŠ¤í¬ë¦°ìƒ· 2023-12-19 18.18.54.png"

    if not Path(image_path).exists():
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")
        sys.exit(1)

    print("="*60)
    print("ğŸ§¥ Cloth Segmentation í…ŒìŠ¤íŠ¸")
    print("="*60)
    print()

    # ì„¸ê·¸ë©˜í…Œì´ì…˜ ìˆ˜í–‰
    segmentation = SimpleClothSegmentation()
    items = segmentation.segment_image(image_path)

    if not items:
        print("\nâš ï¸  ì˜ë¥˜ ì•„ì´í…œì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ê²°ê³¼ ì €ì¥
    segmentation.save_results(items)

    # ìš”ì•½
    print(f"\n{'='*60}")
    print("ğŸ“‹ ìš”ì•½:")
    print(f"   - ì…ë ¥: {image_path}")
    print(f"   - ê°ì§€ëœ ì•„ì´í…œ: {len(items)}ê°œ")
    for item in items:
        print(f"     â€¢ {item['label']} ({item['category']}): {item['opaque_ratio']*100:.1f}% ë¶ˆíˆ¬ëª…")
    print("="*60)


if __name__ == "__main__":
    main()
