"""
RabbitMQ Cloth Processing Worker
- RabbitMQì—ì„œ ì˜· ì²˜ë¦¬ ìš”ì²­ì„ ë°›ì•„ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
- rembg â†’ segmentation â†’ imagen expansion â†’ inpainting
- ê²°ê³¼ë¥¼ RabbitMQë¡œ ë‹¤ì‹œ ì „ì†¡
"""

import pika
import json
import base64
import os
import sys
import traceback
from pathlib import Path
from PIL import Image
import io
import torch
import numpy as np
from rembg import remove
from datetime import datetime
from dotenv import load_dotenv

# .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
# í˜„ì¬ ë””ë ‰í† ë¦¬ë¶€í„° ìƒìœ„ë¡œ ìë™ ê²€ìƒ‰ (í”„ë¡œì íŠ¸ ë£¨íŠ¸ì˜ .env ì°¾ìŒ)
load_dotenv()

# Google AI Imagen ì„œë¹„ìŠ¤ import
try:
    from ..services.imagen_service import GoogleAIImagenService
    IMAGEN_AVAILABLE = True
except ImportError:
    IMAGEN_AVAILABLE = False
    print("âš ï¸  Google AI Imagen ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    print("   ì„¤ì¹˜: pip install google-cloud-aiplatform")

# ê¸°ì¡´ segmentation ëª¨ë¸ import
try:
    from ..api.cloth_segmentation_api import ClothSegmentationModel
    USE_EXISTING_MODEL = True
except ImportError:
    from transformers import AutoImageProcessor, AutoModelForSemanticSegmentation
    USE_EXISTING_MODEL = False

# U2NET ëª¨ë¸ import (ë‹¨ì¼ ì˜· ì´ë¯¸ì§€ìš©)
try:
    from ..utils.u2net_process import load_seg_model, get_palette, generate_mask
    U2NET_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸  U2NET ëª¨ë¸ import ì‹¤íŒ¨: {e}")
    U2NET_AVAILABLE = False

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬
PROJECT_ROOT = Path(__file__).parent.parent
OUTPUTS_DIR = PROJECT_ROOT / "outputs"
SEGMENTED_DIR = OUTPUTS_DIR / "segmented_clothes"
REMOVED_BG_DIR = OUTPUTS_DIR / "removed_bg"
EXPANDED_DIR = OUTPUTS_DIR / "expanded"  # Nano Banana í™•ì¥ë§Œ ëœ ì´ë¯¸ì§€
INPAINTED_DIR = OUTPUTS_DIR / "inpainted"

# ë””ë ‰í† ë¦¬ ìƒì„±
SEGMENTED_DIR.mkdir(parents=True, exist_ok=True)
REMOVED_BG_DIR.mkdir(parents=True, exist_ok=True)
EXPANDED_DIR.mkdir(parents=True, exist_ok=True)
INPAINTED_DIR.mkdir(parents=True, exist_ok=True)

# RabbitMQ ì„¤ì •
RABBITMQ_HOST = os.getenv("RABBITMQ_HOST", "localhost")
RABBITMQ_PORT = int(os.getenv("RABBITMQ_PORT", "5672"))
RABBITMQ_USER = os.getenv("RABBITMQ_USERNAME", "guest")
RABBITMQ_PASS = os.getenv("RABBITMQ_PASSWORD", "guest")

REQUEST_QUEUE = "cloth.processing.queue"
RESULT_QUEUE = "cloth.result.queue"
PROGRESS_QUEUE = "cloth.progress.queue"
EXCHANGE = "cloth.exchange"
REQUEST_ROUTING_KEY = "cloth.processing"
RESULT_ROUTING_KEY = "cloth.result"
PROGRESS_ROUTING_KEY = "cloth.progress"

# ì˜ìƒ ì¹´í…Œê³ ë¦¬ ì •ì˜
CLOTH_LABELS = {
    0: "background",
    1: "hat",
    2: "hair",
    3: "sunglasses",
    4: "upper-clothes",
    5: "skirt",
    6: "pants",
    7: "dress",
    8: "belt",
    9: "shoes",  # left-shoe + right-shoe í†µí•©
    10: "right-shoe",  # ì‚¬ìš©ë˜ì§€ ì•ŠìŒ (9ë¡œ í†µí•©ë¨)
    11: "face",
    12: "left-leg",
    13: "right-leg",
    14: "left-arm",
    15: "right-arm",
    16: "bag",
    17: "scarf"
}

# ì¹´í…Œê³ ë¦¬ ë§¤í•‘ (AI ë¼ë²¨ â†’ ì•± ì¹´í…Œê³ ë¦¬)
# Spring Category enum: ACC, TOP, BOTTOM, SHOES
CATEGORY_MAPPING = {
    "upper-clothes": "TOP",
    "dress": "TOP",  # ì›í”¼ìŠ¤ë„ ìƒì˜ë¡œ ë¶„ë¥˜
    "pants": "BOTTOM",
    "skirt": "BOTTOM",
    "hat": "ACC",
    "bag": "ACC",
    "scarf": "ACC",
    "shoes": "SHOES"  # left-shoe + right-shoe í†µí•©
}


class ClothProcessingPipeline:
    """ì˜· ì´ë¯¸ì§€ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""

    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸš€ Initializing pipeline on device: {self.device}")

        # Segmentation ëª¨ë¸ ë¡œë“œ
        if USE_EXISTING_MODEL:
            print("  Using existing ClothSegmentationModel")
            self.seg_model = ClothSegmentationModel()
        else:
            print("  Using direct transformers model")
            model_name = "mattmdjaga/segformer_b2_clothes"
            from transformers import AutoImageProcessor, AutoModelForSemanticSegmentation
            self.processor = AutoImageProcessor.from_pretrained(model_name)
            self.model = AutoModelForSemanticSegmentation.from_pretrained(model_name)
            self.model.to(self.device)
            self.model.eval()

        # U2NET ëª¨ë¸ ë¡œë“œ (ë‹¨ì¼ ì˜· ì´ë¯¸ì§€ìš©)
        self.u2net_available = False
        if U2NET_AVAILABLE:
            try:
                checkpoint_path = Path(__file__).parent.parent / "model" / "cloth_segm.pth"
                if checkpoint_path.exists():
                    self.u2net_model = load_seg_model(str(checkpoint_path), device=str(self.device))
                    self.u2net_palette = get_palette(4)
                    self.u2net_available = True
                    print("  âœ… U2NET ëª¨ë¸ ë¡œë“œ ì„±ê³µ (ë‹¨ì¼ ì˜· ì´ë¯¸ì§€ìš©)")
                else:
                    print(f"  âš ï¸  U2NET ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ: {checkpoint_path}")
            except Exception as e:
                print(f"  âš ï¸  U2NET ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")

        # Google AI Imagen ì„œë¹„ìŠ¤ ì´ˆê¸°í™” (ì„ íƒì )
        if IMAGEN_AVAILABLE:
            try:
                self.imagen_service = GoogleAIImagenService()
                self.use_imagen = True
                print("  âœ… Google AI Imagen ì„œë¹„ìŠ¤ í™œì„±í™”")
            except Exception as e:
                print(f"  âš ï¸  Google AI Imagen ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                print("     ì´ë¯¸ì§€ í™•ì¥ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
                self.use_imagen = False
        else:
            self.use_imagen = False

        print("âœ… Pipeline initialized successfully")

    def remove_background(self, image_bytes):
        """ë°°ê²½ ì œê±° (rembg)"""
        print("  Step 1/3: Removing background...")
        output = remove(image_bytes)
        image = Image.open(io.BytesIO(output)).convert("RGBA")
        print("  âœ… Background removed")
        return image

    def segment_clothing(self, image):
        """ì˜ìƒ ì„¸ê·¸ë©˜í…Œì´ì…˜ (ì—¬ëŸ¬ ì˜ë¥˜ ê°ì§€)"""
        print("  Step 2/3: Segmenting clothing with Segformer (ì „ì‹  ì‚¬ì§„ ëª¨ë“œ)...")
        print("  ğŸ¤– ëª¨ë¸: Hugging Face SegFormer (Multi-class segmentation)")

        # RGBë¡œ ë³€í™˜ (segmentation ëª¨ë¸ì€ RGB ì‚¬ìš©)
        if image.mode == "RGBA":
            rgb_image = Image.new("RGB", image.size, (255, 255, 255))
            rgb_image.paste(image, mask=image.split()[3])
            image = rgb_image
        elif image.mode != "RGB":
            image = image.convert("RGB")

        if USE_EXISTING_MODEL:
            # ê¸°ì¡´ ëª¨ë¸ ì‚¬ìš©
            pred_seg = self.seg_model.segment_image(image)
        else:
            # ì§ì ‘ ëª¨ë¸ ì‚¬ìš©
            inputs = self.processor(
                images=image,
                return_tensors="pt",
                do_rescale=True,
                do_normalize=True
            )
            inputs = {k: v.to(self.device) for k, v in inputs.items()}

            with torch.no_grad():
                outputs = self.model(**inputs)

            logits = outputs.logits
            upsampled_logits = torch.nn.functional.interpolate(
                logits,
                size=image.size[::-1],
                mode="bilinear",
                align_corners=False
            )

            pred_seg = upsampled_logits.argmax(dim=1)[0].cpu().numpy()

        # ì˜ìƒ ì•„ì´í…œ ì¶”ì¶œ (ëª¨ë“  ì˜ë¥˜)
        detected_items = self._extract_cloth_items(image, pred_seg)

        if not detected_items:
            raise Exception("No clothing items detected in image")

        # ëª¨ë“  ê°ì§€ëœ ì˜ë¥˜ ì¶œë ¥
        print(f"  ğŸ“‹ Detected {len(detected_items)} clothing item(s):")
        for item in detected_items:
            print(f"     - {item['label']}: {item['area_pixels']} pixels")

        # ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ë©”ì¸ ì•„ì´í…œ ì„ íƒ
        # ìš°ì„ ìˆœìœ„: dress > upper-clothes > pants > skirt > shoes > bag > hat
        PRIORITY_ORDER = ["dress", "upper-clothes", "pants", "skirt", "left-shoe", "right-shoe", "bag", "hat", "scarf"]

        primary_item = None
        for priority_label in PRIORITY_ORDER:
            for item in detected_items:
                if item["label"] == priority_label:
                    primary_item = item
                    break
            if primary_item:
                break

        # ìš°ì„ ìˆœìœ„ì— ì—†ìœ¼ë©´ ê°€ì¥ í° ê²ƒ ì„ íƒ
        if not primary_item:
            primary_item = max(detected_items, key=lambda x: x["area_pixels"])

        print(f"  âœ… Primary item selected: {primary_item['label']} ({primary_item['area_pixels']} pixels)")

        # ëª¨ë“  ê°ì§€ëœ ì•„ì´í…œë„ í•¨ê»˜ ë°˜í™˜
        return primary_item, detected_items

    def segment_clothing_u2net(self, image):
        """U2NET ëª¨ë¸ë¡œ ë‹¨ì¼ ì˜· ì´ë¯¸ì§€ ì„¸ê·¸ë©˜í…Œì´ì…˜ - ë°°ê²½ ì œê±°, ì˜·ë§Œ ì¶”ì¶œ"""
        print("  Step 2/3: Segmenting clothing with U2NET (ë‹¨ì¼ ì˜· ëª¨ë“œ)...")
        print("  ğŸ¤– ëª¨ë¸: U2NET (Single item segmentation + Auto category detection)")

        # RGBë¡œ ë³€í™˜
        if image.mode == "RGBA":
            rgb_image = Image.new("RGB", image.size, (255, 255, 255))
            rgb_image.paste(image, mask=image.split()[3])
            image = rgb_image
        elif image.mode != "RGB":
            image = image.convert("RGB")

        # U2NETìœ¼ë¡œ ë§ˆìŠ¤í¬ ìƒì„±
        cloth_mask = generate_mask(image, self.u2net_model, self.u2net_palette, device=str(self.device))

        # ë§ˆìŠ¤í¬ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
        mask_np = np.array(cloth_mask)
        image_np = np.array(image)

        # U2NET ì¹´í…Œê³ ë¦¬ ë§¤í•‘ (ìë™ ê°ì§€)
        U2NET_LABEL_MAP = {
            1: "upper-clothes",  # ìƒì˜
            2: "pants",          # í•˜ì˜
            3: "dress"           # ì›í”¼ìŠ¤
        }

        # ê° í´ë˜ìŠ¤ë³„ í”½ì…€ ìˆ˜ ê³„ì‚°
        unique_labels = np.unique(mask_np)
        class_pixels = {}
        for label_id in unique_labels:
            if label_id == 0:  # ë°°ê²½ ì œì™¸
                continue
            pixel_count = np.sum(mask_np == label_id)
            class_pixels[label_id] = pixel_count

        # ë°°ê²½ ì œì™¸í•˜ê³  ê°€ì¥ ë§ì€ í”½ì…€ì„ ì°¨ì§€í•˜ëŠ” í´ë˜ìŠ¤ ì°¾ê¸° (ì¹´í…Œê³ ë¦¬ ìë™ ê°ì§€)
        if not class_pixels:
            raise Exception("No clothing detected in single item image")

        dominant_class = max(class_pixels, key=class_pixels.get)
        detected_label = U2NET_LABEL_MAP.get(dominant_class, "upper-clothes")
        print(f"     ğŸ¯ ìë™ ê°ì§€ëœ ì¹´í…Œê³ ë¦¬: {detected_label} (U2NET class {dominant_class})")

        # ë°°ê²½ì´ ì•„ë‹Œ ëª¨ë“  ì˜ì—­ì„ ì˜·ìœ¼ë¡œ ê°„ì£¼ (ëª¨ë“  ì˜· í´ë˜ìŠ¤ í†µí•©)
        cloth_mask_binary = (mask_np > 0).astype(np.uint8)
        total_cloth_pixels = np.sum(cloth_mask_binary)

        # 1. ì›ë³¸ í¬ê¸° ìœ ì§€ ë²„ì „ ìƒì„± (ë°°ê²½ë§Œ íˆ¬ëª…)
        alpha_channel_full = (cloth_mask_binary * 255).astype(np.uint8)
        image_rgba_full = np.dstack([image_np, alpha_channel_full])
        fullsize_pil = Image.fromarray(image_rgba_full, mode='RGBA')

        print(f"     ğŸ–¼ï¸  ì›ë³¸ í¬ê¸° ìœ ì§€: {image.size} (WÃ—H), ì˜· ì˜ì—­: {total_cloth_pixels:,} pixels")

        # 2. íƒ€ì´íŠ¸ í¬ë¡­ ë²„ì „ ìƒì„± (ì˜· ì˜ì—­ë§Œ)
        rows = np.any(cloth_mask_binary, axis=1)
        cols = np.any(cloth_mask_binary, axis=0)

        if not rows.any() or not cols.any():
            raise Exception("No clothing detected in single item image")

        rmin, rmax = np.where(rows)[0][[0, -1]]
        cmin, cmax = np.where(cols)[0][[0, -1]]

        # Adaptive padding
        bbox_width = cmax - cmin
        bbox_height = rmax - rmin
        adaptive_padding = max(5, min(20, int(max(bbox_width, bbox_height) * 0.02)))

        img_height, img_width = image_np.shape[:2]
        rmin_padded = max(0, rmin - adaptive_padding)
        rmax_padded = min(img_height - 1, rmax + adaptive_padding)
        cmin_padded = max(0, cmin - adaptive_padding)
        cmax_padded = min(img_width - 1, cmax + adaptive_padding)

        # í¬ë¡­
        mask_cropped = cloth_mask_binary[rmin_padded:rmax_padded+1, cmin_padded:cmax_padded+1]
        image_cropped = image_np[rmin_padded:rmax_padded+1, cmin_padded:cmax_padded+1]

        # RGBA ì´ë¯¸ì§€ ìƒì„± (í¬ë¡­ ë²„ì „)
        alpha_channel_cropped = (mask_cropped * 255).astype(np.uint8)
        image_rgba_cropped = np.dstack([image_cropped, alpha_channel_cropped])
        cropped_pil = Image.fromarray(image_rgba_cropped, mode='RGBA')

        # í’ˆì§ˆ í™•ì¸
        opaque_ratio = np.sum(alpha_channel_cropped > 0) / alpha_channel_cropped.size
        print(f"     âœ‚ï¸  í¬ë¡­ í¬ê¸°: {image_rgba_cropped.shape[1]}Ã—{image_rgba_cropped.shape[0]}, ë¶ˆíˆ¬ëª… ë¹„ìœ¨: {opaque_ratio*100:.1f}%")

        if opaque_ratio < 0.3:
            print(f"     âš ï¸  ê²½ê³ : íˆ¬ëª… ì˜ì—­ì´ {(1-opaque_ratio)*100:.1f}%ë¡œ ê³¼ë„í•©ë‹ˆë‹¤.")
        elif opaque_ratio > 0.8:
            print(f"     âœ… ì–‘í˜¸: í¬ë¡­ì´ ì˜ ë˜ì—ˆìŠµë‹ˆë‹¤.")

        # Primary ì•„ì´í…œ ê²°ê³¼ (í¬ë¡­ ë²„ì „ ì‚¬ìš©)
        primary_item = {
            "label": detected_label,  # U2NETìœ¼ë¡œ ìë™ ê°ì§€ëœ ì¹´í…Œê³ ë¦¬
            "label_id": 4,  # Spring Bootì—ì„œ ì‚¬ìš©í•˜ëŠ” ID (ë§¤í•‘ í•„ìš”)
            "area_pixels": int(total_cloth_pixels),
            "bbox": [int(cmin_padded), int(rmin_padded), int(cmax_padded), int(rmax_padded)],
            "cropped_image": cropped_pil,  # í¬ë¡­ ë²„ì „
            "fullsize_image": fullsize_pil  # ì›ë³¸ í¬ê¸° ìœ ì§€ ë²„ì „ (ìƒˆë¡œ ì¶”ê°€)
        }

        print(f"  âœ… Single item detected: {detected_label} ({total_cloth_pixels:,} pixels)")
        print(f"     - í¬ë¡­ ë²„ì „: {image_rgba_cropped.shape[1]}Ã—{image_rgba_cropped.shape[0]}")
        print(f"     - Fullsize ë²„ì „: {image.size}")

        return primary_item, [primary_item]  # ë‹¨ì¼ ì•„ì´í…œì´ë¯€ë¡œ ë¦¬ìŠ¤íŠ¸ì—ë„ ê°™ì€ ì•„ì´í…œ

    def _extract_cloth_items(self, image, segmentation_map):
        """ì„¸ê·¸ë©˜í…Œì´ì…˜ ë§µì—ì„œ ì˜ìƒ ì•„ì´í…œ ì¶”ì¶œ (ë§ˆìŠ¤í¬ ì ìš©, íˆ¬ëª… ë°°ê²½)"""
        # ì‹ ë°œ í†µí•©: left-shoe (9) + right-shoe (10) â†’ shoes (9)
        # segmentation_mapì„ ë³µì‚¬í•˜ì—¬ ìˆ˜ì • (ì›ë³¸ ë³´ì¡´)
        segmentation_map = segmentation_map.copy()

        left_shoe_exists = 9 in segmentation_map
        right_shoe_exists = 10 in segmentation_map

        if left_shoe_exists or right_shoe_exists:
            print(f"     ğŸ‘Ÿ ì‹ ë°œ ê°ì§€: left={left_shoe_exists}, right={right_shoe_exists}")
            # left-shoeì™€ right-shoeë¥¼ ëª¨ë‘ label 9 (shoes)ë¡œ í†µí•©
            segmentation_map[segmentation_map == 10] = 9
            print(f"     ğŸ‘Ÿ ì‹ ë°œ í†µí•© ì™„ë£Œ: left-shoe + right-shoe â†’ shoes")

        unique_labels = np.unique(segmentation_map)
        results = []
        image_np = np.array(image)
        img_height, img_width = image_np.shape[:2]

        # ì œì™¸í•  ë¼ë²¨: ë¨¸ë¦¬ì¹´ë½, ì–¼êµ´, íŒ”/ë‹¤ë¦¬
        EXCLUDE_LABELS = {2, 11, 12, 13, 14, 15}  # hair, face, left-leg, right-leg, left-arm, right-arm

        # Padding ì„¤ì • (bounding box ì—¬ìœ  ê³µê°„)
        PADDING = 5  # í”½ì…€ ë‹¨ìœ„ (ë„ˆë¬´ í¬ë©´ íˆ¬ëª… ì˜ì—­ì´ ê³¼ë„í•˜ê²Œ í¬í•¨ë¨)

        for label_id in unique_labels:
            if label_id == 0:  # background ì œì™¸
                continue

            # ë¨¸ë¦¬ì¹´ë½, ì–¼êµ´, íŒ”/ë‹¤ë¦¬ ëª…ì‹œì ìœ¼ë¡œ ì œì™¸
            if label_id in EXCLUDE_LABELS:
                continue

            label_name = CLOTH_LABELS.get(label_id, "unknown")

            # ì˜· ê´€ë ¨ ë¼ë²¨ë§Œ ì²˜ë¦¬
            if label_name not in CATEGORY_MAPPING:
                continue

            # ë§ˆìŠ¤í¬ ìƒì„±
            mask = (segmentation_map == label_id).astype(np.uint8)
            area_pixels = np.sum(mask)

            # ë„ˆë¬´ ì‘ì€ ì˜ì—­ ì œì™¸ (ë…¸ì´ì¦ˆ)
            if area_pixels < 1000:
                print(f"     âš ï¸  {label_name} ì˜ì—­ì´ ë„ˆë¬´ ì‘ì•„ ì œì™¸ë¨ ({area_pixels} pixels)")
                continue

            # Bounding box ê³„ì‚°
            rows = np.any(mask, axis=1)
            cols = np.any(mask, axis=0)

            if not rows.any() or not cols.any():
                print(f"     âš ï¸  {label_name} bounding box ê³„ì‚° ì‹¤íŒ¨")
                continue

            rmin, rmax = np.where(rows)[0][[0, -1]]
            cmin, cmax = np.where(cols)[0][[0, -1]]

            # Adaptive Padding (ì˜· í¬ê¸°ì— ë¹„ë¡€)
            bbox_width = cmax - cmin
            bbox_height = rmax - rmin

            # ì˜· í¬ê¸°ì˜ 2%ë¥¼ paddingìœ¼ë¡œ ì‚¬ìš© (ìµœì†Œ 5, ìµœëŒ€ 20)
            adaptive_padding = max(5, min(20, int(max(bbox_width, bbox_height) * 0.02)))

            # Padding ì¶”ê°€ (ì´ë¯¸ì§€ ê²½ê³„ ë‚´ì—ì„œ)
            rmin_padded = max(0, rmin - adaptive_padding)
            rmax_padded = min(img_height - 1, rmax + adaptive_padding)
            cmin_padded = max(0, cmin - adaptive_padding)
            cmax_padded = min(img_width - 1, cmax + adaptive_padding)

            print(f"     ğŸ“ {label_name} bbox: ({cmin}, {rmin}) â†’ ({cmax}, {rmax}) [í¬ê¸°: {bbox_width}Ã—{bbox_height}]")
            print(f"        â†’ padding: {adaptive_padding}px, ìµœì¢…: ({cmin_padded}, {rmin_padded}) â†’ ({cmax_padded}, {rmax_padded})")

            # ========== ë§ˆìŠ¤í¬ ì ìš© í¬ë¡­ (íˆ¬ëª… ë°°ê²½) ==========
            # Paddingì´ ì ìš©ëœ ì˜ì—­ìœ¼ë¡œ ë§ˆìŠ¤í¬ ë° ì´ë¯¸ì§€ í¬ë¡­
            mask_cropped = mask[rmin_padded:rmax_padded+1, cmin_padded:cmax_padded+1]
            image_cropped = image_np[rmin_padded:rmax_padded+1, cmin_padded:cmax_padded+1]

            # RGBë¡œ ë³€í™˜ (í•„ìš”ì‹œ)
            if len(image_cropped.shape) == 2:  # Grayscale
                image_cropped = np.stack([image_cropped] * 3, axis=-1)
            elif image_cropped.shape[2] == 4:  # RGBA â†’ RGB
                # í°ìƒ‰ ë°°ê²½ì— í•©ì„±
                rgb_image = np.ones((image_cropped.shape[0], image_cropped.shape[1], 3), dtype=np.uint8) * 255
                alpha = image_cropped[:, :, 3:4] / 255.0
                rgb_image = (image_cropped[:, :, :3] * alpha + rgb_image * (1 - alpha)).astype(np.uint8)
                image_cropped = rgb_image

            # RGBA ì´ë¯¸ì§€ ìƒì„± (ë§ˆìŠ¤í¬ ì˜ì—­ë§Œ ë¶ˆíˆ¬ëª…, ë‚˜ë¨¸ì§€ëŠ” íˆ¬ëª…)
            alpha_channel = (mask_cropped * 255).astype(np.uint8)
            image_rgba = np.dstack([image_cropped, alpha_channel])

            # ë””ë²„ê·¸: í¬ë¡­ëœ ì´ë¯¸ì§€ í¬ê¸° ë° í’ˆì§ˆ í™•ì¸
            opaque_ratio = np.sum(alpha_channel > 0) / alpha_channel.size
            print(f"     ğŸ–¼ï¸  {label_name} í¬ë¡­ ê²°ê³¼: {image_rgba.shape} (HÃ—WÃ—C), ë¶ˆíˆ¬ëª… ë¹„ìœ¨: {opaque_ratio*100:.1f}%")

            if opaque_ratio < 0.3:
                print(f"     âš ï¸  ê²½ê³ : íˆ¬ëª… ì˜ì—­ì´ {(1-opaque_ratio)*100:.1f}%ë¡œ ê³¼ë„í•©ë‹ˆë‹¤. í¬ë¡­ì´ ë„ˆë¬´ ë„“ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            elif opaque_ratio > 0.8:
                print(f"     âœ… ì–‘í˜¸: í¬ë¡­ì´ ì˜ ë˜ì—ˆìŠµë‹ˆë‹¤.")

            cropped_pil = Image.fromarray(image_rgba, mode='RGBA')
            # ==================================================

            results.append({
                "label": label_name,
                "label_id": int(label_id),
                "area_pixels": int(area_pixels),
                "bbox": [int(cmin_padded), int(rmin_padded), int(cmax_padded), int(rmax_padded)],
                "cropped_image": cropped_pil
            })

        return results

    def inpaint_image(self, image):
        """ì´ë¯¸ì§€ ë³µì› (ê°„ë‹¨í•œ ì²˜ë¦¬ - Stable Diffusionì€ ë„ˆë¬´ ë¬´ê±°ìš°ë¯€ë¡œ ìƒëµ ê°€ëŠ¥)"""
        print("  Step 3/3: Inpainting image...")
        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ì›ë³¸ ì´ë¯¸ì§€ë¥¼ ë°˜í™˜ (ì‹¤ì œë¡œëŠ” Stable Diffusion ì‚¬ìš©)
        # í”„ë¡œë•ì…˜ì—ì„œëŠ” clothing_restorer.pyì˜ ë¡œì§ ì‚¬ìš©
        print("  âš ï¸  Inpainting skipped (use original cropped image)")
        return image

    def process(self, cloth_id, user_id, image_bytes, image_type, worker=None):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        print(f"\nğŸ”„ Processing clothId: {cloth_id}, userId: {user_id}, imageType: {image_type}")

        try:
            # ëª¨ë¸ ì„ íƒ
            use_u2net = (image_type == "SINGLE_ITEM" and self.u2net_available)

            print(f"\n{'='*60}")
            if use_u2net:
                print("  ğŸ¯ ëª¨ë¸ ì„ íƒ: U2NET")
                print("  ğŸ“¸ ì´ë¯¸ì§€ íƒ€ì…: SINGLE_ITEM (ë‹¨ì¼ ì˜· ì´ë¯¸ì§€)")
                print("  ğŸ” ì²˜ë¦¬ ë°©ì‹: ë‹¨ì¼ ì•„ì´í…œ ê°ì§€ + ìë™ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜")
            else:
                print("  ğŸ¯ ëª¨ë¸ ì„ íƒ: Segformer (HuggingFace)")
                print(f"  ğŸ“¸ ì´ë¯¸ì§€ íƒ€ì…: {image_type} (ì „ì‹  ì‚¬ì§„)")
                print("  ğŸ” ì²˜ë¦¬ ë°©ì‹: ë‹¤ì¤‘ ì˜ë¥˜ ì•„ì´í…œ ê°ì§€ (ìƒì˜/í•˜ì˜/ì‹ ë°œ/ê°€ë°© ë“±)")
            print(f"{'='*60}\n")

            # Step 1: Background Removal (0% â†’ 33%)
            if worker:
                worker.send_progress(cloth_id, user_id, "PROCESSING", "ë°°ê²½ ì œê±° ì¤‘...", 10)
            print(f"  [10%] ë°°ê²½ ì œê±° ì¤‘...")
            removed_bg_image = self.remove_background(image_bytes)
            removed_bg_path = REMOVED_BG_DIR / f"{cloth_id}.png"
            removed_bg_image.save(removed_bg_path)
            if worker:
                worker.send_progress(cloth_id, user_id, "PROCESSING", "ë°°ê²½ ì œê±° ì™„ë£Œ", 33)
            print(f"  [33%] ë°°ê²½ ì œê±° ì™„ë£Œ")

            # Step 2: Cloth Segmentation (33% â†’ 66%)
            if worker:
                worker.send_progress(cloth_id, user_id, "PROCESSING", "ì˜· ì˜ì—­ ë¶„ì„ ì¤‘...", 40)
            print(f"  [40%] ì˜· ì˜ì—­ ë¶„ì„ ì¤‘...")

            if use_u2net:
                # U2NET ëª¨ë¸ ì‚¬ìš© (ë‹¨ì¼ ì˜· ì´ë¯¸ì§€)
                primary_item, all_detected_items = self.segment_clothing_u2net(removed_bg_image)

                # U2NET: fullsize ë²„ì „ë„ ì €ì¥ (ë°°ê²½ë§Œ íˆ¬ëª…, ì›ë³¸ í¬ê¸° ìœ ì§€)
                if "fullsize_image" in primary_item:
                    fullsize_image = primary_item["fullsize_image"]
                    fullsize_path = SEGMENTED_DIR / f"{cloth_id}_fullsize.png"
                    fullsize_image.save(fullsize_path)
                    print(f"  ğŸ’¾ Fullsize ì´ë¯¸ì§€ ì €ì¥: {fullsize_path}")
            else:
                # Segformer ëª¨ë¸ ì‚¬ìš© (ì „ì‹  ì‚¬ì§„)
                primary_item, all_detected_items = self.segment_clothing(removed_bg_image)

            # ëª¨ë“  ê°ì§€ëœ ì•„ì´í…œ ì €ì¥ (í¬ë¡­ ë²„ì „) - í”½ì…€ í¬ê¸°ìˆœ ì •ë ¬
            all_detected_items_sorted = sorted(all_detected_items, key=lambda x: x["area_pixels"], reverse=True)

            # Primary ì•„ì´í…œì€ ê°€ì¥ í° ê²ƒ
            primary_item = all_detected_items_sorted[0]
            cropped_image = primary_item["cropped_image"]
            segmented_path = SEGMENTED_DIR / f"{cloth_id}.png"
            cropped_image.save(segmented_path)
            print(f"  ğŸ’¾ ë©”ì¸ í¬ë¡­ ì´ë¯¸ì§€ ì €ì¥: {segmented_path} ({primary_item['label']}, {primary_item['area_pixels']} pixels)")

            # ëª¨ë“  ì•„ì´í…œì˜ í¬ë¡­ ë²„ì „ ì €ì¥ (ì¶”ê°€ ì•„ì´í…œë“¤)
            all_segmented_items = []
            all_segmented_items.append({
                "label": primary_item["label"],
                "segmentedPath": str(segmented_path.absolute()),
                "areaPixels": primary_item["area_pixels"]
            })

            additional_count = len(all_detected_items_sorted) - 1
            for idx, item in enumerate(all_detected_items_sorted[1:], 1):  # primary ì œì™¸
                item_segmented_path = SEGMENTED_DIR / f"{cloth_id}_{item['label']}.png"
                item["cropped_image"].save(item_segmented_path)
                print(f"  ğŸ’¾ ì¶”ê°€ í¬ë¡­ ì´ë¯¸ì§€ ì €ì¥: {item_segmented_path} ({item['label']}, {item['area_pixels']} pixels)")

                all_segmented_items.append({
                    "label": item["label"],
                    "segmentedPath": str(item_segmented_path.absolute()),
                    "areaPixels": item["area_pixels"]
                })

            print(f"  ğŸ’¾ Found {len(all_detected_items)} clothing item(s) (1 primary + {additional_count} additional)")

            if worker:
                worker.send_progress(cloth_id, user_id, "PROCESSING", "ì˜· ì˜ì—­ ë¶„ì„ ì™„ë£Œ", 66)
            print(f"  [66%] ì˜· ì˜ì—­ ë¶„ì„ ì™„ë£Œ")

            # Step 3: Google AI Imagen í™•ì¥ - Primary item (66% â†’ 70%)
            if self.use_imagen:
                if worker:
                    worker.send_progress(cloth_id, user_id, "PROCESSING", "ë©”ì¸ ì•„ì´í…œ í™•ì¥ ì¤‘...", 68)
                print(f"  [68%] ë©”ì¸ ì•„ì´í…œ Nano Banana í™•ì¥ ì¤‘...")
                cropped_image = self.imagen_service.expand_image(cropped_image)

                # Nano Banana í™•ì¥ ê²°ê³¼ ì €ì¥
                expanded_path = EXPANDED_DIR / f"{cloth_id}.png"
                cropped_image.save(expanded_path)
                print(f"  [69%] ë©”ì¸ ì•„ì´í…œ Nano Banana í™•ì¥ ì´ë¯¸ì§€ ì €ì¥: {expanded_path}")
                print(f"  [70%] ë©”ì¸ ì•„ì´í…œ Nano Banana í™•ì¥ ì™„ë£Œ")
            else:
                print(f"  [68%] Nano Banana ë¹„í™œì„±í™” - ë©”ì¸ ì•„ì´í…œ í™•ì¥ ê±´ë„ˆëœ€")

            # Step 4: Inpainting - Primary item (70% â†’ 75%)
            if worker:
                worker.send_progress(cloth_id, user_id, "PROCESSING", "ë©”ì¸ ì•„ì´í…œ ë³µì› ì¤‘...", 72)
            print(f"  [72%] ë©”ì¸ ì•„ì´í…œ ì¸í˜ì¸íŒ… ì¤‘...")
            inpainted_image = self.inpaint_image(cropped_image)
            inpainted_path = INPAINTED_DIR / f"{cloth_id}.png"
            inpainted_image.save(inpainted_path)
            print(f"  [75%] ë©”ì¸ ì•„ì´í…œ ë³µì› ì™„ë£Œ")

            # Step 4: Gemini í™•ì¥ - Additional items (75% â†’ 95%)
            progress = 75
            progress_step = 20 / max(len(all_detected_items_sorted), 1)  # ë‚¨ì€ 20%ë¥¼ ëª¨ë“  ì•„ì´í…œë“¤ë¡œ ë¶„ë°°

            all_expanded_items = []

            # Primary itemë„ expanded_itemsì— ì¶”ê°€
            all_expanded_items.append({
                "label": primary_item["label"],
                "expandedPath": str(expanded_path.absolute()) if self.use_imagen else str(segmented_path.absolute()),
                "areaPixels": primary_item["area_pixels"]
            })

            # ì¶”ê°€ ì•„ì´í…œë“¤ ì²˜ë¦¬ (í¬ê¸°ìˆœìœ¼ë¡œ ì •ë ¬ëœ ìˆœì„œëŒ€ë¡œ)
            for idx, item in enumerate(all_detected_items_sorted[1:], 1):  # primary ì œì™¸
                item_image = item["cropped_image"]

                # Google AI Imagen í™•ì¥ (ë¬´ì¡°ê±´ ì²˜ë¦¬)
                if self.use_imagen:
                    if worker:
                        worker.send_progress(
                            cloth_id, user_id, "PROCESSING",
                            f"ì¶”ê°€ ì•„ì´í…œ í™•ì¥ ì¤‘... ({idx}/{additional_count})",
                            int(progress)
                        )
                    print(f"  [{int(progress)}%] ì¶”ê°€ ì•„ì´í…œ Nano Banana í™•ì¥ ì¤‘... ({idx}/{additional_count}): {item['label']}")
                    item_image = self.imagen_service.expand_image(item_image)

                    # Nano Banana í™•ì¥ ê²°ê³¼ ì €ì¥
                    expanded_item_path = EXPANDED_DIR / f"{cloth_id}_{item['label']}.png"
                    item_image.save(expanded_item_path)
                    print(f"  [{int(progress)}%] ì¶”ê°€ ì•„ì´í…œ Nano Banana í™•ì¥ ì´ë¯¸ì§€ ì €ì¥: {expanded_item_path}")

                    all_expanded_items.append({
                        "label": item["label"],
                        "expandedPath": str(expanded_item_path.absolute()),
                        "areaPixels": item["area_pixels"]
                    })
                else:
                    # Gemini ë¹„í™œì„±í™” ì‹œ í¬ë¡­ ì´ë¯¸ì§€ ê²½ë¡œ ì‚¬ìš©
                    item_segmented_path = SEGMENTED_DIR / f"{cloth_id}_{item['label']}.png"
                    all_expanded_items.append({
                        "label": item["label"],
                        "expandedPath": str(item_segmented_path.absolute()),
                        "areaPixels": item["area_pixels"]
                    })
                    print(f"  [{int(progress)}%] Nano Banana ë¹„í™œì„±í™” - ì¶”ê°€ ì•„ì´í…œ í™•ì¥ ê±´ë„ˆëœ€: {item['label']}")

                # ì¸í˜ì¸íŒ… (í˜„ì¬ëŠ” ê·¸ëƒ¥ í™•ì¥ëœ ì´ë¯¸ì§€ ë°˜í™˜)
                # inpainted = expandedì´ë¯€ë¡œ ë³„ë„ ì €ì¥ ë¶ˆí•„ìš”

                progress += progress_step

            if worker:
                worker.send_progress(cloth_id, user_id, "PROCESSING", "ëª¨ë“  ì•„ì´í…œ í™•ì¥ ì™„ë£Œ", 95)
            print(f"  [95%] ëª¨ë“  ì•„ì´í…œ í™•ì¥ ì™„ë£Œ (ì´ {len(all_expanded_items)}ê°œ)")

            # ì¹´í…Œê³ ë¦¬ ë§¤í•‘
            suggested_category = CATEGORY_MAPPING.get(primary_item["label"], "ACC")  # ê¸°ë³¸ê°’: ACC

            result = {
                "clothId": cloth_id,
                "success": True,
                "errorMessage": None,
                "removedBgImagePath": str(removed_bg_path.absolute()),
                "segmentedImagePath": str(segmented_path.absolute()),  # primaryë§Œ (í•˜ìœ„ í˜¸í™˜)
                "inpaintedImagePath": str(inpainted_path.absolute()),  # primaryë§Œ (í•˜ìœ„ í˜¸í™˜)
                "suggestedCategory": suggested_category,
                "segmentationLabel": primary_item["label"],
                "areaPixels": primary_item["area_pixels"],
                # ìƒˆë¡œìš´ í•„ë“œ: ëª¨ë“  ì•„ì´í…œ (í¬ê¸°ìˆœ ì •ë ¬)
                "allSegmentedItems": all_segmented_items,  # ëª¨ë“  í¬ë¡­ ì´ë¯¸ì§€
                "allExpandedItems": all_expanded_items,    # ëª¨ë“  Gemini í™•ì¥ ì´ë¯¸ì§€
                # í•˜ìœ„ í˜¸í™˜ìš© (deprecated)
                "additionalClothingItems": [
                    {"label": item["label"], "path": item["expandedPath"], "area_pixels": item["areaPixels"]}
                    for item in all_expanded_items[1:]  # primary ì œì™¸
                ]
            }

            print(f"\n{'='*60}")
            print(f"âœ… Processing completed: {suggested_category}")
            print(f"   ğŸ¤– ì‚¬ìš©ëœ ëª¨ë¸: {'U2NET' if use_u2net else 'Segformer'}")
            print(f"   ğŸ“¸ ì´ë¯¸ì§€ íƒ€ì…: {image_type}")
            print(f"   ğŸ“¦ ê°ì§€ëœ ì•„ì´í…œ: {len(all_segmented_items)}ê°œ (Segmented)")
            print(f"   ğŸ¨ í™•ì¥ëœ ì•„ì´í…œ: {len(all_expanded_items)}ê°œ (Imagen)")
            print(f"   ğŸ·ï¸  Primary ì¹´í…Œê³ ë¦¬: {suggested_category} ({primary_item['label']})")
            print(f"{'='*60}\n")
            return result

        except Exception as e:
            print(f"âŒ Processing failed: {str(e)}")
            traceback.print_exc()

            return {
                "clothId": cloth_id,
                "success": False,
                "errorMessage": str(e),
                "removedBgImagePath": None,
                "segmentedImagePath": None,
                "inpaintedImagePath": None,
                "suggestedCategory": None,
                "segmentationLabel": None,
                "areaPixels": None
            }


class ClothProcessingWorker:
    """RabbitMQ Worker"""

    def __init__(self):
        self.pipeline = ClothProcessingPipeline()
        self.connection = None
        self.channel = None
        self.user_id = None  # í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ ì‚¬ìš©ì ID

    def connect(self):
        """RabbitMQ ì—°ê²°"""
        credentials = pika.PlainCredentials(RABBITMQ_USER, RABBITMQ_PASS)
        parameters = pika.ConnectionParameters(
            host=RABBITMQ_HOST,
            port=RABBITMQ_PORT,
            credentials=credentials,
            heartbeat=600,
            blocked_connection_timeout=300
        )

        self.connection = pika.BlockingConnection(parameters)
        self.channel = self.connection.channel()

        # Exchange ì„ ì–¸
        self.channel.exchange_declare(
            exchange=EXCHANGE,
            exchange_type='direct',
            durable=True
        )

        # í ì„ ì–¸
        self.channel.queue_declare(queue=REQUEST_QUEUE, durable=True)
        self.channel.queue_declare(queue=RESULT_QUEUE, durable=True)
        self.channel.queue_declare(queue=PROGRESS_QUEUE, durable=True)

        # ë°”ì¸ë”©
        self.channel.queue_bind(
            exchange=EXCHANGE,
            queue=REQUEST_QUEUE,
            routing_key=REQUEST_ROUTING_KEY
        )
        self.channel.queue_bind(
            exchange=EXCHANGE,
            queue=RESULT_QUEUE,
            routing_key=RESULT_ROUTING_KEY
        )
        self.channel.queue_bind(
            exchange=EXCHANGE,
            queue=PROGRESS_QUEUE,
            routing_key=PROGRESS_ROUTING_KEY
        )

        # QoS ì„¤ì • (í•œ ë²ˆì— 1ê°œë§Œ ì²˜ë¦¬)
        self.channel.basic_qos(prefetch_count=1)

        print(f"âœ… Connected to RabbitMQ at {RABBITMQ_HOST}:{RABBITMQ_PORT}")

    def on_message(self, ch, method, properties, body):
        """ë©”ì‹œì§€ ìˆ˜ì‹  ì½œë°±"""
        try:
            # ë©”ì‹œì§€ íŒŒì‹±
            message = json.loads(body)
            cloth_id = message["clothId"]
            user_id = message.get("userId")  # userId ì¶”ì¶œ
            image_bytes_data = message["imageBytes"]
            original_filename = message["originalFilename"]
            image_type = message.get("imageType", "FULL_BODY")  # imageType ì¶”ì¶œ (ê¸°ë³¸ê°’: FULL_BODY)

            print(f"\nğŸ“¨ Received message: clothId={cloth_id}, userId={user_id}, filename={original_filename}, imageType={image_type}")

            # userId ì €ì¥ (ì§„í–‰ë„ ì „ì†¡ì— í•„ìš”)
            self.user_id = user_id

            # imageBytes ì²˜ë¦¬ (Jacksonì´ byte[]ë¥¼ ìˆ«ì ë°°ì—´ë¡œ ì§ë ¬í™”í•¨)
            if isinstance(image_bytes_data, str):
                # Base64 ë¬¸ìì—´ì¸ ê²½ìš°
                image_bytes = base64.b64decode(image_bytes_data)
            elif isinstance(image_bytes_data, list):
                # ìˆ«ì ë°°ì—´ì¸ ê²½ìš° (Jackson ê¸°ë³¸ ì§ë ¬í™”)
                image_bytes = bytes(image_bytes_data)
            else:
                raise ValueError(f"Unsupported imageBytes format: {type(image_bytes_data)}")

            # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (worker ì¸ìŠ¤í„´ìŠ¤ ì „ë‹¬í•˜ì—¬ ì§„í–‰ë„ ì „ì†¡ ê°€ëŠ¥í•˜ê²Œ)
            result = self.pipeline.process(cloth_id, user_id, image_bytes, image_type, self)

            # ê²°ê³¼ ì „ì†¡
            self.send_result(result)

            # ACK
            ch.basic_ack(delivery_tag=method.delivery_tag)
            print(f"âœ… Message processed and acknowledged\n")

        except Exception as e:
            print(f"âŒ Error processing message: {str(e)}")
            traceback.print_exc()

            # NACK (ì¬ì‹œë„)
            ch.basic_nack(delivery_tag=method.delivery_tag, requeue=True)

    def send_progress(self, cloth_id, user_id, status, current_step, progress_percentage):
        """ì§„í–‰ë„ ë©”ì‹œì§€ ì „ì†¡"""
        progress_message = {
            "clothId": cloth_id,
            "userId": user_id,
            "status": status,
            "currentStep": current_step,
            "progressPercentage": progress_percentage,
            "timestamp": int(datetime.now().timestamp() * 1000)
        }

        progress_json = json.dumps(progress_message)

        self.channel.basic_publish(
            exchange=EXCHANGE,
            routing_key=PROGRESS_ROUTING_KEY,
            body=progress_json,
            properties=pika.BasicProperties(
                delivery_mode=2,
                content_type='application/json'
            )
        )

        print(f"ğŸ“Š Progress sent: {progress_percentage}% - {current_step}")

    def send_result(self, result):
        """ê²°ê³¼ ë©”ì‹œì§€ ì „ì†¡"""
        result_json = json.dumps(result)

        self.channel.basic_publish(
            exchange=EXCHANGE,
            routing_key=RESULT_ROUTING_KEY,
            body=result_json,
            properties=pika.BasicProperties(
                delivery_mode=2,  # persistent
                content_type='application/json'
            )
        )

        print(f"ğŸ“¤ Result sent to {RESULT_QUEUE}")

    def start(self):
        """Worker ì‹œì‘"""
        self.connect()

        print(f"ğŸ¯ Listening on queue: {REQUEST_QUEUE}")
        print("Waiting for messages. To exit press CTRL+C\n")

        self.channel.basic_consume(
            queue=REQUEST_QUEUE,
            on_message_callback=self.on_message
        )

        try:
            self.channel.start_consuming()
        except KeyboardInterrupt:
            print("\nâ›” Stopping worker...")
            self.channel.stop_consuming()
        finally:
            if self.connection:
                self.connection.close()
            print("ğŸ‘‹ Worker stopped")


if __name__ == "__main__":
    worker = ClothProcessingWorker()
    worker.start()
